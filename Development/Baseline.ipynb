{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import sklearn\n",
    "import pyphen\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_score(gold_labels, predicted_labels, detailed=False):\n",
    "    macro_F1 = sklearn.metrics.f1_score(gold_labels, predicted_labels, average='macro')\n",
    "    print(\"macro-F1: {:.2f}\".format(macro_F1))\n",
    "    if detailed:\n",
    "        scores = sklearn.metrics.precision_recall_fscore_support(gold_labels, predicted_labels)\n",
    "        print(\"{:^10}{:^10}{:^10}{:^10}{:^10}\".format(\"Label\", \"Precision\", \"Recall\", \"F1\", \"Support\"))\n",
    "        print('-' * 50)\n",
    "        print(\"{:^10}{:^10.2f}{:^10.2f}{:^10.2f}{:^10}\".format(0, scores[0][0], scores[1][0], scores[2][0], scores[3][0]))\n",
    "        print(\"{:^10}{:^10.2f}{:^10.2f}{:^10.2f}{:^10}\".format(1, scores[0][1], scores[1][1], scores[2][1], scores[3][1]))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def get_score(gold_labels, predicted_labels): \n",
    "    macro_F1 = sklearn.metrics.f1_score(gold_labels, predicted_labels, average='macro')\n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "\n",
    "        trainset_path = \"../datasets/{}/{}_Train.tsv\".format(language, language.capitalize())\n",
    "        devset_path = \"../datasets/{}/{}_Dev.tsv\".format(language, language.capitalize())\n",
    "\n",
    "        self.trainset = self.read_dataset(trainset_path)\n",
    "        self.devset = self.read_dataset(devset_path)\n",
    "\n",
    "    def read_dataset(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            fieldnames = ['hit_id', 'sentence', 'start_offset', 'end_offset', 'target_word', 'native_annots',\n",
    "                          'nonnative_annots', 'native_complex', 'nonnative_complex', 'gold_label', 'gold_prob']\n",
    "            \n",
    "            dataset = pd.read_csv(file, names = fieldnames, sep = \"\\t\")\n",
    "\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import scipy\n",
    "\n",
    "class Baseline(object):\n",
    "\n",
    "    def __init__(self, language, model):\n",
    "        self.language = language\n",
    "        # from 'Multilingual and Cross-Lingual Complex Word Identification' (Yimam et. al, 2017)\n",
    "        if language == 'english':\n",
    "            self.avg_word_length = 5.3\n",
    "        else:  # spanish\n",
    "            self.avg_word_length = 6.2\n",
    "            \n",
    "        self.d = pyphen.Pyphen(lang='en')\n",
    "        #self.dv = DictVectorizer(sparse = False)\n",
    "\n",
    "        if model == \"dtc\":\n",
    "            self.model = DecisionTreeClassifier(random_state=0)\n",
    "        elif model == \"nb\":\n",
    "            self.model = GaussianNB()\n",
    "            \n",
    "        elif model == \"lr\":\n",
    "            self.model = LogisticRegression()\n",
    "            \n",
    "        else:\n",
    "            print(\"Error: choose model\")\n",
    "            \n",
    "    def extract_word_features(self, word, *args):\n",
    "        # here are my basic features:\n",
    "        # - len chars = word length\n",
    "        # - len tokens = phrase length\n",
    "        # - len uniq =  ratio of unique characters in word\n",
    "        # - len vowels = ratio of vowels in word\n",
    "        # - len const = ratio of constonants in word\n",
    "        # - len syl = number of syllables\n",
    "        \n",
    "        # - final baseline system uses tokens, uniq, and const based on feature analyis\n",
    "        \n",
    "        len_chars = len(word) / self.avg_word_length\n",
    "        len_tokens = len(word.split(' '))\n",
    "        len_uniq = len(set(word))/len(word)\n",
    "        len_vowels = len([letter for letter in word.split() if letter in set(\"aeiou\")])/len(word)\n",
    "        len_const = len([letter for letter in word.split() if letter not in set(\"aeiou\")])/len(word)\n",
    "        len_syl = len(self.d.inserted(word).split(\"-\"))\n",
    "   \n",
    "        # dictionary to store the features in, in order to access later when testing individual features\n",
    "        \n",
    "        features_dict = {\"chars\":len_chars,\"tokens\": len_tokens, \"unique\": len_uniq,\n",
    "                    \"vowels\": len_vowels, \"const\":len_const, \"syl\": len_syl,}\n",
    "            \n",
    "        if args:\n",
    "            features = []\n",
    "            for f in args:\n",
    "                features.append(features_dict[f])\n",
    "            return features\n",
    "        else:\n",
    "            features = [len_chars, len_tokens, len_uniq,\n",
    "                    len_vowels, len_const,  len_syl,]\n",
    "            return features\n",
    "    \n",
    "\n",
    "        \n",
    "    def train(self, trainset, *args):\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for idx,sent in trainset.iterrows():\n",
    "            X.append(self.extract_word_features(sent['target_word'], *args))\n",
    "            y.append(sent['gold_label'])\n",
    "            \n",
    "        #X = self.dv.fit_transform(X)\n",
    "            \n",
    "        return self.model.fit(X, y)\n",
    "        \n",
    "    def test(self, testset, *args):\n",
    "        X = []\n",
    "\n",
    "        y = []\n",
    "        \n",
    "        for idx,sent in testset.iterrows():\n",
    "            X.append(self.extract_word_features(sent['target_word'], *args))\n",
    "            y.append(sent['gold_label'])\n",
    "        \n",
    "        #X = self.dv.transform(X)\n",
    "\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def feature_importances(self):\n",
    "        return self.model.feature_importances_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(language, model, feature_analysis = False, *args):\n",
    "    model = Baseline(language, model)\n",
    "    data = Dataset(language)\n",
    "        \n",
    "    if feature_analysis == True:\n",
    "        features= [\"chars\", \"tokens\", \"unique\", \n",
    "                    \"vowels\", \"const\", \"syl\"]\n",
    "        f1 = []\n",
    "        for f in features:\n",
    "            model.train(data.trainset, f)\n",
    "            predictions = model.test(data.devset, f)\n",
    "            gold_labels = data.devset['gold_label']\n",
    "            f1.append(get_score(gold_labels, predictions))\n",
    "        return f1\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"{}: {} training - {} dev\".format(language, len(data.trainset), len(data.devset)))\n",
    "\n",
    "        model.train(data.trainset, *args)\n",
    "\n",
    "        predictions = model.test(data.devset, *args)\n",
    "\n",
    "        gold_labels = data.devset['gold_label']\n",
    "\n",
    "        report_score(gold_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ dtc ------\n",
      "------ nb ------\n",
      "------ lr ------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()   \n",
    "stats = []\n",
    "for model in [\"dtc\", \"nb\", \"lr\"]:\n",
    "    print(\"------ %s ------\" %model)\n",
    "    stats.append(run_model('english', model = model, feature_analysis = True))\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Logistic Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chars</th>\n",
       "      <td>0.708899</td>\n",
       "      <td>0.588843</td>\n",
       "      <td>0.666493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>0.574645</td>\n",
       "      <td>0.574992</td>\n",
       "      <td>0.574992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0.662108</td>\n",
       "      <td>0.615873</td>\n",
       "      <td>0.621597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vowels</th>\n",
       "      <td>0.368810</td>\n",
       "      <td>0.369113</td>\n",
       "      <td>0.368261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>0.697302</td>\n",
       "      <td>0.670847</td>\n",
       "      <td>0.671383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syl</th>\n",
       "      <td>0.660097</td>\n",
       "      <td>0.571551</td>\n",
       "      <td>0.660327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Decision Tree Classifier  Naive Bayes  Logistic Regression\n",
       "chars                   0.708899     0.588843             0.666493\n",
       "tokens                  0.574645     0.574992             0.574992\n",
       "unique                  0.662108     0.615873             0.621597\n",
       "vowels                  0.368810     0.369113             0.368261\n",
       "const                   0.697302     0.670847             0.671383\n",
       "syl                     0.660097     0.571551             0.660327"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(stats, index = [\"Decision Tree Classifier\", \"Naive Bayes\", \"Logistic Regression\"],\n",
    "                  columns = [\"chars\",\"tokens\", \"unique\",\"vowels\", \"const\", \"syl\"]).T\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ dtc ------\n",
      "english: 27299 training - 3328 dev\n",
      "macro-F1: 0.71\n",
      "\n",
      "spanish: 13750 training - 1622 dev\n",
      "macro-F1: 0.72\n",
      "\n",
      "------ nb ------\n",
      "english: 27299 training - 3328 dev\n",
      "macro-F1: 0.66\n",
      "\n",
      "spanish: 13750 training - 1622 dev\n",
      "macro-F1: 0.65\n",
      "\n",
      "------ lr ------\n",
      "english: 27299 training - 3328 dev\n",
      "macro-F1: 0.69\n",
      "\n",
      "spanish: 13750 training - 1622 dev\n",
      "macro-F1: 0.72\n",
      "\n",
      "10.550925016403198\n"
     ]
    }
   ],
   "source": [
    "start = time.time()   \n",
    "3\n",
    "for model in [\"dtc\", \"nb\", \"lr\"]:\n",
    "    features = []\n",
    "    print(\"------ %s ------\" %model)\n",
    "    run_model('english', model = model)\n",
    "    run_model('spanish', model = model)\n",
    "    \n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
