{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import sklearn\n",
    "import pyphen\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_score(gold_labels, predicted_labels, detailed=False):\n",
    "    macro_F1 = sklearn.metrics.f1_score(gold_labels, predicted_labels, average='macro')\n",
    "    print(\"macro-F1: {:.2f}\".format(macro_F1))\n",
    "    if detailed:\n",
    "        scores = sklearn.metrics.precision_recall_fscore_support(gold_labels, predicted_labels)\n",
    "        print(\"{:^10}{:^10}{:^10}{:^10}{:^10}\".format(\"Label\", \"Precision\", \"Recall\", \"F1\", \"Support\"))\n",
    "        print('-' * 50)\n",
    "        print(\"{:^10}{:^10.2f}{:^10.2f}{:^10.2f}{:^10}\".format(0, scores[0][0], scores[1][0], scores[2][0], scores[3][0]))\n",
    "        print(\"{:^10}{:^10.2f}{:^10.2f}{:^10.2f}{:^10}\".format(1, scores[0][1], scores[1][1], scores[2][1], scores[3][1]))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def get_score(gold_labels, predicted_labels): \n",
    "    macro_F1 = sklearn.metrics.f1_score(gold_labels, predicted_labels, average='macro')\n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "\n",
    "        trainset_path = \"../datasets/{}/{}_Train.tsv\".format(language, language.capitalize())\n",
    "        devset_path = \"../datasets/{}/{}_Dev.tsv\".format(language, language.capitalize())\n",
    "\n",
    "        self.trainset = self.read_dataset(trainset_path)\n",
    "        self.devset = self.read_dataset(devset_path)\n",
    "\n",
    "    def read_dataset(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            fieldnames = ['hit_id', 'sentence', 'start_offset', 'end_offset', 'target_word', 'native_annots',\n",
    "                          'nonnative_annots', 'native_complex', 'nonnative_complex', 'gold_label', 'gold_prob']\n",
    "            \n",
    "            dataset = pd.read_csv(file, names = fieldnames, sep = \"\\t\")\n",
    "\n",
    "            #dataset = [sent for sent\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adapted from \n",
    "# https://opendevincode.wordpress.com/2015/08/01/building-a-custom-python-scikit-learn-transformer-for-machine-learning/\n",
    "# and http://michelleful.github.io/code-blog/2015/06/20/pipelines/\n",
    "# http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    \n",
    "    def fit(self, X, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "class Affix_Extractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, affix_type):\n",
    "        self.affix_type = affix_type\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        result = []\n",
    "        for word in X:\n",
    "            if self.affix_type == \"prefix\":\n",
    "                affix = word[:3]\n",
    "            elif self.affix_type == \"suffix\":\n",
    "                affix = word[-3:]\n",
    "                \n",
    "            row_dict = {affix:1}\n",
    "            result.append(row_dict)\n",
    "        return result\n",
    "            \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "\n",
    "class WordFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    # here are my basic features:\n",
    "        # - len chars = word length\n",
    "        # - len tokens = phrase length\n",
    "        # - len uniq =  ratio of unique characters in word\n",
    "        # - len vowels = ratio of vowels in word\n",
    "        # - len const = ratio of constonants in word\n",
    "        # - len syl = number of syllables\n",
    "        \n",
    "        # - final baseline system uses tokens, uniq, and const based on feature analyis\n",
    "        \n",
    "    def __init__(self,language):\n",
    "        language = language\n",
    "        # from 'Multilingual and Cross-Lingual Complex Word Identification' (Yimam et. al, 2017)\n",
    "        if language == 'english':\n",
    "            self.avg_word_length = 5.3\n",
    "        else:  # spanish\n",
    "            self.avg_word_length = 6.2\n",
    "            \n",
    "        self.d = pyphen.Pyphen(lang='en')\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        result = []\n",
    "        for word in X:\n",
    "            len_chars = len(word) / self.avg_word_length\n",
    "            len_tokens = len(word.split(' '))\n",
    "            len_uniq = len(set(word))/len(word)\n",
    "            len_vowels = len([letter for letter in word.split() if letter in set(\"aeiou\")])/len(word)\n",
    "            len_const = len([letter for letter in word.split() if letter not in set(\"aeiou\")])/len(word)\n",
    "            len_syl = len(self.d.inserted(word).split(\"-\"))\n",
    "\n",
    "            # dictionary to store the features in, in order to access later when testing individual features\n",
    "            row_dict = {\"chars\":len_chars,\"tokens\": len_tokens, \"unique\": len_uniq,\n",
    "                        \"vowels\": len_vowels, \"const\":len_const, \"syl\": len_syl,}\n",
    "\n",
    "            result.append(row_dict)\n",
    "        return result\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD as PCA\n",
    "import scipy\n",
    "\n",
    "class DTC_Model(object):\n",
    "\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "        # from 'Multilingual and Cross-Lingual Complex Word Identification' (Yimam et. al, 2017)\n",
    "        if language == 'english':\n",
    "            self.avg_word_length = 5.3\n",
    "            self.d = pyphen.Pyphen(lang='en')\n",
    "        else:  # spanish\n",
    "            self.avg_word_length = 6.2\n",
    "            self.d = pyphen.Pyphen(lang='es')\n",
    "            \n",
    "        \n",
    "        self.dv = DictVectorizer()\n",
    "        self.cv = CountVectorizer(ngram_range = (1,3))\n",
    "        #self.cv = TfidfVectorizer(ngram_range = (2,2))\n",
    "        self.model = DecisionTreeClassifier(class_weight = \"balanced\", random_state=0)\n",
    "        self.build_pipe()\n",
    "        \n",
    "    def build_pipe(self):\n",
    "        word_features = Pipeline([('select', Selector(key=\"target_word\"))] +\n",
    "            [( 'wfe', WordFeatureExtractor(self.language) )] +\n",
    "            [( 'dv', DictVectorizer() )])\n",
    "\n",
    "        Ngrams = Pipeline([('select', Selector(key = \"sentence\"))]+\n",
    "            [('cv', CountVectorizer())])\n",
    "        \n",
    "        suffix = Pipeline([('select', Selector(key='target_word'))]+\n",
    "                          [('suf',Affix_Extractor(affix_type = 'suffix'))]+\n",
    "                          [( 'dv', DictVectorizer())])\n",
    "        \n",
    "        prefix = Pipeline([('select', Selector(key='target_word'))]+\n",
    "                          [('suf',Affix_Extractor(affix_type = 'prefix'))]+\n",
    "                          [( 'dv', DictVectorizer())])\n",
    "        \n",
    "        char_ngrams = Pipeline([('select', Selector(key = 'target_word'))]+\n",
    "                              [('cv', CountVectorizer(analyzer = 'char_wb',ngram_range=(2, 3)))])\n",
    "\n",
    "        \n",
    "        f_union = Pipeline([('union', FeatureUnion(transformer_list = [\n",
    "                            ('words', word_features),('ngrams', Ngrams),('sffx', suffix), ('pffx', prefix),('char', char_ngrams) ]))])\n",
    "        #('char', char_ngrams)\n",
    "        \n",
    "        self.pipe = f_union\n",
    "        \n",
    "\n",
    "    def extract_word_features(self, word, *args):\n",
    "        # here are my basic features:\n",
    "        # - len chars = word length\n",
    "        # - len tokens = phrase length\n",
    "        # - len uniq =  ratio of unique characters in word\n",
    "        # - len vowels = ratio of vowels in word\n",
    "        # - len const = ratio of constonants in word\n",
    "        # - len syl = number of syllables\n",
    "        \n",
    "        # - final baseline system uses tokens, uniq, and const based on feature analyis\n",
    "        \n",
    "        len_chars = len(word) / self.avg_word_length\n",
    "        len_tokens = len(word.split(' '))\n",
    "        len_uniq = len(set(word))/len(word)\n",
    "        len_vowels = len([letter for letter in word.split() if letter in set(\"aeiou\")])/len(word)\n",
    "        len_const = len([letter for letter in word.split() if letter not in set(\"aeiou\")])/len(word)\n",
    "        len_syl = len(self.d.inserted(word).split(\"-\"))\n",
    "   \n",
    "        # dictionary to store the features in, in order to access later when testing individual features\n",
    "        features_dict = {\"chars\":len_chars,\"tokens\": len_tokens, \"unique\": len_uniq,\n",
    "                    \"vowels\": len_vowels, \"const\":len_const, \"syl\": len_syl,}\n",
    "            \n",
    "        return features_dict\n",
    "    \n",
    "    def extract_context_features(self, sent):\n",
    "        \n",
    "        ## update this feature properly\n",
    "        sentence = sent['sentence']\n",
    "        word = sent['target_word']\n",
    "        sentence = [sentence.replace(word,\"\")]\n",
    "        return(self.cv.transform(sentence))\n",
    "\n",
    "\n",
    "    def get_features(self, trainset):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def train(self, trainset, *args):\n",
    "        X = self.pipe.fit_transform(trainset)\n",
    "        y = trainset['gold_label']\n",
    "       \n",
    "        return self.model.fit(X, y)\n",
    "        \n",
    "    def test(self, testset, *args):\n",
    "        X = self.pipe.transform(testset)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def feature_importances(self):\n",
    "        return self.model.feature_importances_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(language, *args):\n",
    "    data = Dataset(language)\n",
    "    print(\"{}: {} training - {} dev\".format(language, len(data.trainset), len(data.devset)))\n",
    "\n",
    "    model = DTC_Model(language)\n",
    "\n",
    "    model.train(data.trainset, *args)\n",
    "\n",
    "\n",
    "    predictions = model.test(data.devset, *args)\n",
    "    \n",
    "    gold_labels = data.devset['gold_label']\n",
    "\n",
    "    #gold_labels = [sent['gold_label'] for sent in data.devset]\n",
    "\n",
    "    report_score(gold_labels, predictions, detailed = True)\n",
    "    \n",
    "    #print(model.feature_importances())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english: 27299 training - 3328 dev\n",
      "macro-F1: 0.77\n",
      "  Label   Precision   Recall      F1     Support  \n",
      "--------------------------------------------------\n",
      "    0        0.80      0.81      0.81      1940   \n",
      "    1        0.73      0.72      0.73      1388   \n",
      "\n",
      "spanish: 13750 training - 1622 dev\n",
      "macro-F1: 0.70\n",
      "  Label   Precision   Recall      F1     Support  \n",
      "--------------------------------------------------\n",
      "    0        0.75      0.78      0.76      969    \n",
      "    1        0.65      0.62      0.63      653    \n",
      "\n",
      "26.44724988937378\n"
     ]
    }
   ],
   "source": [
    "start = time.time()   \n",
    "\n",
    "run_model('english',)\n",
    "run_model('spanish',)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ab', 'ac', 'ad', 'ae']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import itertools\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "alphabet = \"abcde\"\n",
    "char_bigrams = [ x[0]+x[1] for x in itertools.product(alphabet, repeat = 2)]\n",
    "print(char_bigrams[:5])\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(char_bigrams)\n",
    "cv.get_params()\n",
    "\n",
    "x = cv.transform([\"aa ab\"])\n",
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x2 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = DictVectorizer()\n",
    "x = [{\"chars\":6,\"syl\":3},{\"chars\":3, \"syl\": 1}]\n",
    "dv.fit(x)\n",
    "\n",
    "dv.transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV = CountVectorizer()\n",
    "words_1 = [\"hello world\"]\n",
    "words_2 = [\"Hi dan\"]\n",
    "CV.fit(words_1)\n",
    "CV.fit(words_2)\n",
    "words = words_1 + words_2\n",
    "\n",
    "sent = [\"The barren islands, reefs and coral outcrops are believed to be in rich in oil and gas and the overlapping claims have long been feared as Asia's next flashpoint for armed conflict.\"]\n",
    "\n",
    "y = CV.transform(words).toarray()\n",
    "y = CV.fit_transform(sent).toarray()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "class WordFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    # here are my basic features:\n",
    "        # - len chars = word length\n",
    "        # - len tokens = phrase length\n",
    "        # - len uniq =  ratio of unique characters in word\n",
    "        # - len vowels = ratio of vowels in word\n",
    "        # - len const = ratio of constonants in word\n",
    "        # - len syl = number of syllables\n",
    "        \n",
    "        # - final baseline system uses tokens, uniq, and const based on feature analyis\n",
    "        \n",
    "    def __init__(self,language):\n",
    "        language = language\n",
    "        # from 'Multilingual and Cross-Lingual Complex Word Identification' (Yimam et. al, 2017)\n",
    "        if language == 'english':\n",
    "            self.avg_word_length = 5.3\n",
    "        else:  # spanish\n",
    "            self.avg_word_length = 6.2\n",
    "            \n",
    "        self.d = pyphen.Pyphen(lang='en')\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        result = []\n",
    "        for word in X:\n",
    "            len_chars = len(word) / self.avg_word_length\n",
    "            len_tokens = len(word.split(' '))\n",
    "            len_uniq = len(set(word))/len(word)\n",
    "            len_vowels = len([letter for letter in word.split() if letter in set(\"aeiou\")])/len(word)\n",
    "            len_const = len([letter for letter in word.split() if letter not in set(\"aeiou\")])/len(word)\n",
    "            len_syl = len(self.d.inserted(word).split(\"-\"))\n",
    "\n",
    "            # dictionary to store the features in, in order to access later when testing individual features\n",
    "            row_dict = {\"chars\":len_chars,\"tokens\": len_tokens, \"unique\": len_uniq,\n",
    "                        \"vowels\": len_vowels, \"const\":len_const, \"syl\": len_syl,}\n",
    "\n",
    "            result.append(row_dict)\n",
    "        return result\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "language = \"english\"\n",
    "data = Dataset(language)\n",
    "\n",
    "#data.trainset['gold_label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Suffix_Extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-807ccb24376d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m suffix = Pipeline([('select', Selector(key='target_word'))]+\n\u001b[0;32m----> 2\u001b[0;31m                   [('suf',Suffix_Extractor())])\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSFX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSFX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Suffix_Extractor' is not defined"
     ]
    }
   ],
   "source": [
    "suffix = Pipeline([('select', Selector(key='target_word'))]+\n",
    "                  [('suf',Suffix_Extractor())])\n",
    "\n",
    "SFX = suffix.fit_transform(data.trainset)\n",
    "print(SFX[:10])\n",
    "LB = CountVectorizer()\n",
    "#LB = LabelBinarizer()\n",
    "print(len(SFX))\n",
    "\n",
    "\n",
    "LB.fit_transform(SFX).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class My_One_Hot(BaseEstimator, TransformerMixin):\n",
    "    def __init(self):\n",
    "        pass\n",
    "    def fit(self, X, *_):\n",
    "        return self\n",
    "    def transform(self,X,*_):\n",
    "        X = X.apply((lambda x: x[-3:]))\n",
    "        result = pd.get_dummies(X)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MOH = My_One_Hot()\n",
    "moh = MOH.fit_transform(data.trainset['target_word'])\n",
    "moh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('select', Selector(key=\"target_word\"))] +\n",
    "    [( 'wfe', WordFeatureExtractor(language) )] +\n",
    "    [( 'dv', DictVectorizer() )])\n",
    "\n",
    "pipeline2 = Pipeline([('select', Selector(key = \"sentence\"))]+\n",
    "    [('cv', CountVectorizer())])\n",
    "\n",
    "\n",
    "union_pipeline = Pipeline([('union', FeatureUnion(transformer_list = [('words', pipeline),('contexts', pipeline2)]))])\n",
    "X = union_pipeline.fit_transform(data.trainset)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hi = \"hello\"\n",
    "hi[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        return df[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = Selector(key = 'target_word')\n",
    "selected = S.fit_transform(data.trainset)\n",
    "for i in range(10):\n",
    "    print(selected[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
