{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import sklearn\n",
    "import pyphen\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import time\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_score(gold_labels, predicted_labels, detailed=False):\n",
    "    macro_F1 = sklearn.metrics.f1_score(gold_labels, predicted_labels, average='macro')\n",
    "    print(\"macro-F1: {:.2f}\".format(macro_F1))\n",
    "    if detailed:\n",
    "        scores = sklearn.metrics.precision_recall_fscore_support(gold_labels, predicted_labels)\n",
    "        print(\"{:^10}{:^10}{:^10}{:^10}{:^10}\".format(\"Label\", \"Precision\", \"Recall\", \"F1\", \"Support\"))\n",
    "        print('-' * 50)\n",
    "        print(\"{:^10}{:^10.2f}{:^10.2f}{:^10.2f}{:^10}\".format(0, scores[0][0], scores[1][0], scores[2][0], scores[3][0]))\n",
    "        print(\"{:^10}{:^10.2f}{:^10.2f}{:^10.2f}{:^10}\".format(1, scores[0][1], scores[1][1], scores[2][1], scores[3][1]))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def get_score(gold_labels, predicted_labels): \n",
    "    macro_F1 = sklearn.metrics.f1_score(gold_labels, predicted_labels, average='macro')\n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "\n",
    "        trainset_path = \"../datasets/{}/{}_Train.tsv\".format(language, language.capitalize())\n",
    "        devset_path = \"../datasets/{}/{}_Dev.tsv\".format(language, language.capitalize())\n",
    "\n",
    "        self.trainset = self.read_dataset(trainset_path)\n",
    "        self.devset = self.read_dataset(devset_path)\n",
    "\n",
    "    def read_dataset(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            fieldnames = ['hit_id', 'sentence', 'start_offset', 'end_offset', 'target_word', 'native_annots',\n",
    "                          'nonnative_annots', 'native_complex', 'nonnative_complex', 'gold_label', 'gold_prob']\n",
    "            \n",
    "            dataset = pd.read_csv(file, names = fieldnames, sep = \"\\t\")\n",
    "\n",
    "            #dataset = [sent for sent\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import scipy\n",
    "\n",
    "class DTC_Model(object):\n",
    "\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "        # from 'Multilingual and Cross-Lingual Complex Word Identification' (Yimam et. al, 2017)\n",
    "        if language == 'english':\n",
    "            self.avg_word_length = 5.3\n",
    "        else:  # spanish\n",
    "            self.avg_word_length = 6.2\n",
    "            \n",
    "        self.d = pyphen.Pyphen(lang='en')\n",
    "        self.dv = DictVectorizer()\n",
    "        self.cv = CountVectorizer(ngram_range = (1,3))\n",
    "        #self.cv = TfidfVectorizer(ngram_range = (2,2))\n",
    "        self.model = DecisionTreeClassifier(class_weight = \"balanced\", random_state=0)\n",
    "\n",
    "    def extract_word_features(self, word, *args):\n",
    "        # here are my basic features:\n",
    "        # - len chars = word length\n",
    "        # - len tokens = phrase length\n",
    "        # - len uniq =  ratio of unique characters in word\n",
    "        # - len vowels = ratio of vowels in word\n",
    "        # - len const = ratio of constonants in word\n",
    "        # - len syl = number of syllables\n",
    "        \n",
    "        # - final baseline system uses tokens, uniq, and const based on feature analyis\n",
    "        \n",
    "        len_chars = len(word) / self.avg_word_length\n",
    "        len_tokens = len(word.split(' '))\n",
    "        len_uniq = len(set(word))/len(word)\n",
    "        len_vowels = len([letter for letter in word.split() if letter in set(\"aeiou\")])/len(word)\n",
    "        len_const = len([letter for letter in word.split() if letter not in set(\"aeiou\")])/len(word)\n",
    "        len_syl = len(self.d.inserted(word).split(\"-\"))\n",
    "   \n",
    "        # dictionary to store the features in, in order to access later when testing individual features\n",
    "        features_dict = {\"chars\":len_chars,\"tokens\": len_tokens, \"unique\": len_uniq,\n",
    "                    \"vowels\": len_vowels, \"const\":len_const, \"syl\": len_syl,}\n",
    "            \n",
    "        return features_dict\n",
    "    \n",
    "    def extract_context_features(self, sent):\n",
    "        \n",
    "        ## update this feature properly\n",
    "        sentence = sent['sentence']\n",
    "        word = sent['target_word']\n",
    "        sentence = [sentence.replace(word,\"\")]\n",
    "        return(self.cv.transform(sentence))\n",
    "\n",
    "\n",
    "    def get_features(self, trainset):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def train(self, trainset, *args):\n",
    "        self.cv.fit(trainset['sentence'])\n",
    "        #self.dv.fit(trainset['target_word'])\n",
    "        word_features = []\n",
    "        context_features= []\n",
    "        y = []\n",
    "        \n",
    "        for idx,sent in trainset.iterrows():\n",
    "            word_features.append(self.extract_word_features(sent['target_word'], *args))\n",
    "            context_features.append(self.extract_context_features(sent))\n",
    "            y.append(sent['gold_label'])\n",
    "        \n",
    "        context_features = scipy.sparse.vstack(context_features)\n",
    "        word_features = self.dv.fit_transform(word_features)\n",
    "        #X = self.dv.fit_transform(X)\n",
    "        X = scipy.sparse.hstack([word_features,context_features])\n",
    "        print(type(X))\n",
    "        #print(type(CF))\n",
    "        return self.model.fit(X, y)\n",
    "        \n",
    "    def test(self, testset, *args):\n",
    "        word_features = []\n",
    "        context_features= []\n",
    "        y = []\n",
    "        \n",
    "        for idx,sent in testset.iterrows():\n",
    "            word_features.append(self.extract_word_features(sent['target_word'], *args))\n",
    "            context_features.append(self.extract_context_features(sent))\n",
    "            y.append(sent['gold_label'])\n",
    "        \n",
    "        context_features = scipy.sparse.vstack(context_features)\n",
    "        word_features = self.dv.fit_transform(word_features)\n",
    "        #X = self.dv.fit_transform(X)\n",
    "        X = scipy.sparse.hstack([word_features,context_features])\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def feature_importances(self):\n",
    "        return self.model.feature_importances_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(language, *args):\n",
    "    data = Dataset(language)\n",
    "    print(\"{}: {} training - {} dev\".format(language, len(data.trainset), len(data.devset)))\n",
    "\n",
    "    model = DTC_Model(language)\n",
    "\n",
    "    model.train(data.trainset, *args)\n",
    "\n",
    "    predictions = model.test(data.devset, *args)\n",
    "    \n",
    "    gold_labels = data.devset['gold_label']\n",
    "\n",
    "    #gold_labels = [sent['gold_label'] for sent in data.devset]\n",
    "\n",
    "    report_score(gold_labels, predictions)\n",
    "    \n",
    "    print(model.feature_importances())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english: 27299 training - 3328 dev\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "macro-F1: 0.71\n",
      "\n",
      "[0.26409978 0.00901217 0.00948946 ... 0.         0.         0.        ]\n",
      "spanish: 13750 training - 1622 dev\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "macro-F1: 0.69\n",
      "\n",
      "[0.08138087 0.01240965 0.01653352 ... 0.         0.00028603 0.        ]\n",
      "72.93032240867615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()   \n",
    "\n",
    "run_model('english',)\n",
    "run_model('spanish',)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ab', 'ac', 'ad', 'ae']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import itertools\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "alphabet = \"abcde\"\n",
    "char_bigrams = [ x[0]+x[1] for x in itertools.product(alphabet, repeat = 2)]\n",
    "print(char_bigrams[:5])\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(char_bigrams)\n",
    "cv.get_params()\n",
    "\n",
    "x = cv.transform([\"aa ab\"])\n",
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x2 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = DictVectorizer()\n",
    "x = [{\"chars\":6,\"syl\":3},{\"chars\":3, \"syl\": 1}]\n",
    "dv.fit(x)\n",
    "\n",
    "dv.transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi my  is dan'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV = CountVectorizer()\n",
    "words_1 = [\"hello world\"]\n",
    "words_2 = [\"Hi dan\"]\n",
    "CV.fit(words_1)\n",
    "CV.fit(words_2)\n",
    "words = words_1 + words_2\n",
    "\n",
    "sent = [\"The barren islands, reefs and coral outcrops are believed to be in rich in oil and gas and the overlapping claims have long been feared as Asia's next flashpoint for armed conflict.\"]\n",
    "\n",
    "y = CV.transform(words).toarray()\n",
    "y = CV.fit_transform(sent).toarray()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'my', 'name', 'is', 'dan']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "class WordFeatureExtractor(TransformerMixin):\n",
    "    # here are my basic features:\n",
    "        # - len chars = word length\n",
    "        # - len tokens = phrase length\n",
    "        # - len uniq =  ratio of unique characters in word\n",
    "        # - len vowels = ratio of vowels in word\n",
    "        # - len const = ratio of constonants in word\n",
    "        # - len syl = number of syllables\n",
    "        \n",
    "        # - final baseline system uses tokens, uniq, and const based on feature analyis\n",
    "        \n",
    "    def __init__(self,language):\n",
    "        language = language\n",
    "        # from 'Multilingual and Cross-Lingual Complex Word Identification' (Yimam et. al, 2017)\n",
    "        if language == 'english':\n",
    "            self.avg_word_length = 5.3\n",
    "        else:  # spanish\n",
    "            self.avg_word_length = 6.2\n",
    "            \n",
    "        self.d = pyphen.Pyphen(lang='en')\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        result = []\n",
    "        for index, rowdata in X.iterrows():\n",
    "            word = rowdata['target_word']\n",
    "            #rowdict = {}\n",
    "\n",
    "            len_chars = len(word) / self.avg_word_length\n",
    "            len_tokens = len(word.split(' '))\n",
    "            len_uniq = len(set(word))/len(word)\n",
    "            len_vowels = len([letter for letter in word.split() if letter in set(\"aeiou\")])/len(word)\n",
    "            len_const = len([letter for letter in word.split() if letter not in set(\"aeiou\")])/len(word)\n",
    "            len_syl = len(self.d.inserted(word).split(\"-\"))\n",
    "\n",
    "            # dictionary to store the features in, in order to access later when testing individual features\n",
    "            rowdict = {\"chars\":len_chars,\"tokens\": len_tokens, \"unique\": len_uniq,\n",
    "                        \"vowels\": len_vowels, \"const\":len_const, \"syl\": len_syl,}\n",
    "\n",
    "            #for kvp in self.kpairs:\n",
    "                #rowdict.update( { rowdata[ kvp[0] ]: rowdata[ kvp[1] ] } )\n",
    "            result.append(rowdict)\n",
    "        return result\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DictVectorizer' object has no attribute 'feature_names_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-5830ab4b8ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: {} training - {} dev\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DictVectorizer' object has no attribute 'feature_names_'"
     ]
    }
   ],
   "source": [
    "\n",
    "language = \"english\"\n",
    "data = Dataset(language)\n",
    "kv = KVExtractor()\n",
    "dv = DictVectorizer()\n",
    "X = kv.transform(data.trainset)\n",
    "X_2 = dv.transform(X)\n",
    "\n",
    "print(\"{}: {} training - {} dev\".format(language, len(data.trainset), len(data.devset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27299x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 136568 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [( 'kv', KVExtractor() )] +\n",
    "    [( 'dv', DictVectorizer() )] +\n",
    "    []\n",
    ")\n",
    "\n",
    "dat = pipeline.fit_transform(data.trainset)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
